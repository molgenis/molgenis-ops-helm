prometheus:
  alertmanager:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi
  alertmanagerFiles:
    alertmanager.yml:
      global:
        slack_api_url: ''
      receivers:
        - name: default-receiver
          slack_configs:
            - channel: '#prometheus'
              send_resolved: true
              text: "{{ range .Alerts }}{{ .Annotations.message }}\n{{ end }}"
      route:
        group_wait: 10s
        group_interval: 5m
        receiver: default-receiver
        repeat_interval: 3h

  kubeStateMetrics:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 64Mi
      requests:
        cpu: 10m
        memory: 64Mi

  nodeExporter:
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 100m
        memory: 30Mi

  pushgateway:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi

  server:
    nodeSelector:
      deployPod: "true"
    extraSecretMounts:
      - name: scrape-secrets
        secretName: scrape-secrets
        mountPath: /etc/secrets
        readOnly: true
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi
  configmapReload:
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 100m
        memory: 64Mi
  serverFiles:
    alerts:
      groups:
        - name: nodes
          rules:
            - alert: "Node Down"
              expr: up{job="kubernetes-nodes"} == 0
              annotations:
                miqTarget: "ContainerNode"
                severity: "ERROR"
                message: "Node {{$labels.instance}} is down"
        - name: pods
          rules:
            - alert: "Pod OOMKilled"
              expr: kube_pod_container_status_terminated_reason{reason='OOMKilled'} == 1
              annotations:
                severity: "WARN"
                message: "Pod {{$labels.container}} in namespace {{$labels.namespace}} got OOMKilled."
        - name: services
          rules:
            - alert: "Many Previews Up"
              expr: sum(kube_service_info{namespace=~"preview.*", service=~".*molgenis"}) > 5
              annotations:
                severity: "DEBUG"
                message: "{{$value}} PR previews are currently deployed"
        - name: molgenis
          rules:
            - alert: "Molgenis Down"
              expr: up{job="molgenis-metrics"} == 0
              annotations:
                severity: "ERROR"
                message: "{{$labels.instance}} is down"
    rules: {}
    prometheus.yml:
      rule_files:
        - /etc/config/rules
        - /etc/config/alerts
      scrape_configs:
        - job_name: jenkins
          metrics_path: '/prometheus/'
          scheme: https
          static_configs:
            - targets: ['jenkins.dev.molgenis.org:443']
        - job_name: 'molgenis-metrics'
          metrics_path: /api/metrics/prometheus
          scheme: https
          static_configs:
            - targets:
                - master.dev.molgenis.org:443
                - latest.test.molgenis.org:443
          basic_auth:
            username: prometheus
            password_file: '/etc/secrets/molgenis-metrics'
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090

        # A scrape configuration for running Prometheus on a Kubernetes cluster.
        # This uses separate scrape configs for cluster components (i.e. API server, node)
        # and services to allow each to use different authentication configs.
        #
        # Kubernetes labels will be added as Prometheus labels on metrics via the
        # `labelmap` relabeling action.

        # Scrape config for API servers.
        #
        # Kubernetes exposes API servers as endpoints to the default/kubernetes
        # service so this uses `endpoints` role and uses relabelling to only keep
        # the endpoints associated with the default/kubernetes service using the
        # default named port `https`. This works for single API server deployments as
        # well as HA API server deployments.
        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

        - job_name: 'kubernetes-nodes'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics


        - job_name: 'kubernetes-nodes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          # This configuration will work only on kubelet 1.7.3+
          # As the scrape endpoints for cAdvisor have changed
          # if you are using older version you need to change the replacement to
          # replacement: /api/v1/nodes/${1}:4194/proxy/metrics
          # more info here https://github.com/coreos/prometheus-operator/issues/633
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            # Only for Kubernetes ^1.7.3.
            # See: https://github.com/prometheus/prometheus/issues/2916
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          metric_relabel_configs:
            - action: replace
              source_labels: [id]
              regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
              target_label: rkt_container_name
              replacement: '${2}-${1}'
            - action: replace
              source_labels: [id]
              regex: '^/system\.slice/(.+)\.service$'
              target_label: systemd_service_name
              replacement: '${1}'

        # Scrape config for service endpoints.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        - job_name: 'kubernetes-service-endpoints'

          kubernetes_sd_configs:
            - role: endpoints

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: kubernetes_name

        - job_name: 'prometheus-pushgateway'
          honor_labels: true

          kubernetes_sd_configs:
            - role: service

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: pushgateway

        # Example scrape config for probing services via the Blackbox Exporter.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/probe`: Only probe services that have a value of `true`
        - job_name: 'kubernetes-services'

          metrics_path: /probe
          params:
            module: [http_2xx]

          kubernetes_sd_configs:
            - role: service

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: true
            - source_labels: [__address__]
              target_label: __param_target
            - target_label: __address__
              replacement: blackbox
            - source_labels: [__param_target]
              target_label: instance
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_service_name]
              target_label: kubernetes_name

        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods'

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name

        - job_name: 'molgenis-blackbox'
          scrape_interval: 300s
          metrics_path: /probe
          params:
            module: [http_2xx, http_3xx]  # Look for a HTTP 2xx and 3xx response.
          file_sd_configs:
            - files:
                - resources/molgenis-blackbox-targets.json
          relabel_configs:
            - source_labels: [__address__]
              target_label: __param_target
            - source_labels: [__param_target]
              target_label: instance
            - target_label: __address__
              replacement: blackbox:9115  # Blackbox exporter.
secret:
  molgenisMetrics: "xxxx"